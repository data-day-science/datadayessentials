name: Deploy Databricks CLuster

on: [workflow_dispatch]
  # pull_request:
  #   branches:
  #     - main
  #   inputs:
  #     deploy_environment:
  #       description: 'choose the environment to deploy to'
  #       required: true
  #       default: 'dev'
    # inputs:
    #   deploy_environment:
    #     description: 'choose the environment to deploy to'
    #     required: true
    #     default: 'dev'

jobs:
  deploy-setup-scripts:
    runs-on: ['self-hosted', 'linux', 'new-runner']
    strategy:
      max-parallel: 4
    environment: dev
    env:
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      DATABRICKS_TOKEN:  ${{ secrets.DATABRICKS_TOKEN }}
      SP_CLIENT_ID: ${{vars.AZURE_CLIENT_ID}}
      SP_CLIENT_ID_KEY: "sp_client_id" 
      SP_TENANT_ID: ${{vars.AZURE_TENANT_ID}}
      SP_TENANT_ID_KEY: "sp_tenant_id" 
      SP_SECRET: ${{secrets.AZURE_CLIENT_SECRET}}
      SP_SECRET_KEY: "sp_secret_key" 
      DBUTILS_SCOPE: 'data_science'
      AZURE_ENVIRONMENT_NAME: ${{vars.AZURE_ENVIRONMENT_NAME}}
      AZURE_SUBSCRIPTION_ID: ${{vars.AZURE_SUBSCRIPTION_ID}}
      AZURE_RESOURCE_GROUP: ${{vars.AZURE_RESOURCE_GROUP}}
      AZURE_ML_WORKSPACE: ${{vars.AZURE_ML_WORKSPACE}}
      AZURE_DATA_LAKE: ${{vars.AZURE_DATA_LAKE}}
      GLOBAL_TOKEN: ${{ secrets.GLOBAL_DATASCIENCE_REPO_PAT }}
      
      

    steps:
      - uses: webfactory/ssh-agent@v0.6.0
        with:
            ssh-private-key: ${{ secrets.CORE_REPO_PRIVATE_KEY }}

      - name: Check out repository code
        uses: actions/checkout@v2

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip' # caching pip dependencies

      - name: Install pip
        run: |
          python -m pip install --upgrade pip

      - name: create ssh key for copying across
        run: |
          echo "${{ secrets.CORE_PAT_TOKEN }}" >> './core_token.txt'
          echo "${{ vars.AZURE_ENVIRONMENT_NAME }}" >> './azure_environment_name.txt'
          echo "${{ vars.AZURE_SUBSCRIPTION_ID }}" >> './azure_subscription_id.txt'
          echo "${{ vars.AZURE_RESOURCE_GROUP }}" >> './azure_resource_group.txt'
          echo "${{ vars.AZURE_ML_WORKSPACE }}" >> './azure_ml_workspace.txt'
          echo "${{ vars.AZURE_DATA_LAKE }}" >> './azure_data_lake.txt'
          echo "${{ vars.AZURE_TENANT_ID }}" >> './azure_tenant_id.txt'
          echo "${{ vars.AZURE_CLIENT_ID }}" >> './azure_client_id.txt'
          echo "${{ secrets.AZURE_CLIENT_SECRET }}" >> './azure_client_secret.txt'
          ls

      - name: Install dependencies and project in dev mode
        run: |
          pip install databricks-cli databricks 

      - name: Upload cluster setup script to databricks dbfs
        run: |
          databricks fs cp --overwrite ./databricks/general_cluster_setup.sh dbfs:/Shared/dbx/cluster_init_scripts/general_cluster_setup.sh
          databricks fs cp --overwrite ./core_token.txt dbfs:/Shared/dbx/cluster_init_scripts/core_token.txt
          databricks fs cp --overwrite ./azure_environment_name.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_environment_name.txt
          databricks fs cp --overwrite ./azure_subscription_id.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_subscription_id.txt
          databricks fs cp --overwrite ./azure_resource_group.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_resource_group.txt
          databricks fs cp --overwrite ./azure_ml_workspace.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_ml_workspace.txt
          databricks fs cp --overwrite ./azure_data_lake.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_data_lake.txt
          databricks fs cp --overwrite ./azure_tenant_id.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_tenant_id.txt
          databricks fs cp --overwrite ./azure_client_secret.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_client_secret.txt
          databricks fs cp --overwrite ./azure_client_id.txt dbfs:/Shared/dbx/cluster_init_scripts/azure_client_id.txt

      - name: set dbutils secrets about storage account
        run: |
          databricks secrets delete-scope --scope ${{env.DBUTILS_SCOPE}} || true
          databricks secrets create-scope --scope ${{env.DBUTILS_SCOPE}} --initial-manage-principal users
          sleep 3
          databricks secrets put --scope ${{env.DBUTILS_SCOPE}} --key ${{env.SP_CLIENT_ID_KEY}} --string-value "${{env.SP_CLIENT_ID}}"
          databricks secrets put --scope ${{env.DBUTILS_SCOPE}} --key ${{env.SP_SECRET_KEY}} --string-value "${{env.SP_SECRET}}"
          databricks secrets put --scope ${{env.DBUTILS_SCOPE}} --key ${{env.SP_TENANT_ID_KEY}} --string-value "${{env.SP_TENANT_ID}}"

      


